## 4.5 Do we Really Need Explicit Position Encodings for Vision Transformers
- 주제
    - 4.5 논문은 Explicit이란 단어가 key word이다. 우리가 명시적으로 positional embedding을 위한 component를 만드는 것이 아니라, __network의 parameter들이 implicit 하게 이를 배울 수 있도록 하는 어떠한 module을 추가하자는 것__ 이 주 idea이다.
    이 module은 Transformer encoder의 출력단에 conv layer 형식으로 추가되는데, 이러한 방식의 근거는 1.2 논문을 따르는 것 같다. __Module이 추가되는 위치가 출력단이라는 것이 중요하다.__ 1.2 논문을 아직 읽어보지는 못했으나, 아마 convolution layer 자체가 position을 encoding하는 능력을 갖고 있다는 얘기가 나올것으로 보인다.(padding과 관련이 있다.) 이것은 다시 원점으로 돌아가 __"Convolution layer의 inductive bias가 positional encoding과 관련이 있다."__ 라는 주장의 근거가 된다. 모든 논문들이 같은 곳을 암묵적으로 가리키고 있는 듯한 느낌을 받는다. __이 내용은 5에 수록된 논문들과 직접적으로 연관될 것으로 보인다. 생각을 정리한 후 5로 넘어가면 많은 것을 얻을 수 있겠다.__ 